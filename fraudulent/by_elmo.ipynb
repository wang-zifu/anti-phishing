{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "!pip install keras==2.2.4 # critical dependency, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import neural network libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from keras import backend as K\n",
    "import keras.layers as layers\n",
    "from keras.models import Model, load_model\n",
    "from keras.engine import Layer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize tensorflow/keras session\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "filepath = \"../data/enron/emails.csv\"\n",
    "\n",
    "# Read the data into a pandas dataframe called emails\n",
    "emails = pd.read_csv(filepath)\n",
    "\n",
    "print(\"Successfully loaded {} rows and {} columns!\".format(emails.shape[0], emails.shape[1]))\n",
    "print(emails.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# take a closer look at the first email\n",
    "print(emails.loc[0][\"message\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import email\n",
    "\n",
    "def extract_messages(df):\n",
    "    messages = []\n",
    "    for item in df[\"message\"]:\n",
    "        # Return a message object structure from a string\n",
    "        e = email.message_from_string(item)    \n",
    "        # get message body  \n",
    "        message_body = e.get_payload()\n",
    "        messages.append(message_body)\n",
    "    print(\"Successfully retrieved message body from e-mails!\")\n",
    "    return messages\n",
    "\n",
    "bodies = extract_messages(emails)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# extract random 10000 enron email bodies for building dataset\n",
    "import random\n",
    "bodies_df = pd.DataFrame(random.sample(bodies, 10000))\n",
    "\n",
    "# expand default pandas display options to make emails more clearly visible when printed\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "\n",
    "bodies_df.head() # you could do print(bodies_df.head()), but Jupyter displays this nicer for pandas DataFrames"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filepath = \"../data/fradulent_emails.txt\"\n",
    "with open(filepath, 'r',encoding=\"latin1\") as file:\n",
    "    data = file.read()\n",
    "    \n",
    "# split on a code word appearing close to the beginning of each email\n",
    "fraud_emails = data.split(\"From r\")\n",
    "\n",
    "print(\"Successfully loaded {} spam emails!\".format(len(fraud_emails)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fraud_bodies = extract_messages(pd.DataFrame(fraud_emails,columns=[\"message\"],dtype=str))\n",
    "fraud_bodies_df = pd.DataFrame(fraud_bodies[1:])\n",
    "\n",
    "fraud_bodies_df.head() # you could do print(fraud_bodies_df.head()), but Jupyter displays this nicer for pandas DataFrames"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Nsamp =1000 # number of samples to generate in each class - 'spam', 'not spam'\n",
    "maxtokens = 200 # the maximum number of tokens per document\n",
    "maxtokenlen = 100 # the maximum length of each token"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tokenize(row):\n",
    "    if row is None or row is '':\n",
    "        tokens = \"\"\n",
    "    else:\n",
    "        tokens = str(row).split(\" \")[:maxtokens]\n",
    "    return tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def reg_expressions(row):\n",
    "    tokens = []\n",
    "    try:\n",
    "        for token in row:\n",
    "            token = token.lower()\n",
    "            token = re.sub(r'[\\W\\d]', \"\", token)\n",
    "            token = token[:maxtokenlen] # truncate token\n",
    "            tokens.append(token)\n",
    "    except:\n",
    "        token = \"\"\n",
    "        tokens.append(token)\n",
    "    return tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')    \n",
    "\n",
    "def stop_word_removal(row):\n",
    "    token = [token for token in row if token not in stopwords]\n",
    "    token = filter(None, token)\n",
    "    return token"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Convert everything to lower-case, truncate to maxtokens and truncate each token to maxtokenlen\n",
    "EnronEmails = bodies_df.iloc[:,0].apply(tokenize)\n",
    "EnronEmails = EnronEmails.apply(stop_word_removal)\n",
    "EnronEmails = EnronEmails.apply(reg_expressions)\n",
    "EnronEmails = EnronEmails.sample(Nsamp)\n",
    "\n",
    "SpamEmails = fraud_bodies_df.iloc[:,0].apply(tokenize)\n",
    "SpamEmails = SpamEmails.apply(stop_word_removal)\n",
    "SpamEmails = SpamEmails.apply(reg_expressions)\n",
    "SpamEmails = SpamEmails.sample(Nsamp)\n",
    "\n",
    "raw_data = pd.concat([SpamEmails,EnronEmails], axis=0).values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Shape of combined data is:\")\n",
    "print(raw_data.shape)\n",
    "print(\"Data represented as numpy array is:\")\n",
    "print(raw_data)\n",
    "\n",
    "# corresponding labels\n",
    "Categories = ['spam','notspam']\n",
    "header = ([1]*Nsamp)\n",
    "header.extend(([0]*Nsamp))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# function for shuffling data\n",
    "def unison_shuffle(a, b):\n",
    "    p = np.random.permutation(len(b))\n",
    "    data = a[p]\n",
    "    header = np.asarray(b)[p]\n",
    "    return data, header\n",
    "\n",
    "# function for converting data into the right format, due to the difference in required format from sklearn models\n",
    "# we expect a single string per email here, versus a list of tokens for the sklearn models previously explored\n",
    "def convert_data(raw_data,header):\n",
    "    converted_data, labels = [], []\n",
    "    for i in range(raw_data.shape[0]):\n",
    "        # combine list of tokens representing each email into single string\n",
    "        out = ' '.join(raw_data[i])\n",
    "        converted_data.append(out)\n",
    "        labels.append(header[i])\n",
    "        #print(i)\n",
    "    converted_data = np.array(converted_data, dtype=object)[:, np.newaxis]\n",
    "    return converted_data, np.array(labels)\n",
    "\n",
    "raw_data, header = unison_shuffle(raw_data, header)\n",
    "\n",
    "# split into independent 70% training and 30% testing sets\n",
    "idx = int(0.7*raw_data.shape[0])\n",
    "# 70% of data for training\n",
    "train_x, train_y = convert_data(raw_data[:idx],header[:idx])\n",
    "# remaining 30% for testing\n",
    "test_x, test_y = convert_data(raw_data[idx:],header[idx:])\n",
    "\n",
    "print(\"train_x/train_y list details, to make sure it is of the right form:\")\n",
    "print(len(train_x))\n",
    "print(train_x)\n",
    "print(train_y[:5])\n",
    "print(train_y.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ElmoEmbeddingLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.dimensions = 1024 # initialize output dimension of ELMo embedding\n",
    "        self.trainable=True\n",
    "        super(ElmoEmbeddingLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape): # function for building ELMo embedding\n",
    "        self.elmo = hub.Module('https://tfhub.dev/google/elmo/2', trainable=self.trainable,\n",
    "                               name=\"{}_module\".format(self.name)) # download pretrained ELMo model\n",
    "        \n",
    "        # extract trainable parameters, which are only a small subset of the total - this is a constraint of\n",
    "        # the tf hub module as shared by the authors - see https://tfhub.dev/google/elmo/2\n",
    "        # the trainable parameters are 4 scalar weights on the sum of the outputs of ELMo layers \n",
    "        self.trainable_weights += K.tf.trainable_variables(scope=\"^{}_module/.*\".format(self.name))\n",
    "        super(ElmoEmbeddingLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None): # specify function for calling embedding\n",
    "        result = self.elmo(K.squeeze(K.cast(x, tf.string), axis=1),\n",
    "                      as_dict=True,\n",
    "                      signature='default',\n",
    "                      )['default']\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape): # specify output shape\n",
    "        return (input_shape[0], self.dimensions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to build overall model\n",
    "def build_model():\n",
    "    input_text = layers.Input(shape=(1,), dtype=\"string\")\n",
    "    embedding = ElmoEmbeddingLayer()(input_text)\n",
    "    dense = layers.Dense(256, activation='relu')(embedding)\n",
    "    pred = layers.Dense(1, activation='sigmoid')(dense) # we could use sigmoid activation as well, but we choose softmax\n",
    "                                                        # to enable us use sparse_categorical_crossentropy and \n",
    "                                                        # sparse_categorical_accuracy below\n",
    "    \n",
    "    model = Model(inputs=[input_text], outputs=pred)\n",
    "    # use sparse_categorical_crossentropy and sparse_categorical_accuracy do avoid having to\n",
    "    # one-hot encode the labels\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build and fit\n",
    "model = build_model()\n",
    "history = model.fit(train_x, \n",
    "          train_y,\n",
    "          validation_data=(test_x, test_y),\n",
    "          epochs=5,\n",
    "          batch_size=32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_history = pd.DataFrame(history.history)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "plt.plot(range(df_history.shape[0]),df_history['val_acc'],'bs--',label='validation')\n",
    "plt.plot(range(df_history.shape[0]),df_history['acc'],'r^--',label='training')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('ELMo Email Classification Training')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "# Save figures\n",
    "fig.savefig('ELMoConvergence.eps', format='eps')\n",
    "fig.savefig('ELMoConvergence.pdf', format='pdf')\n",
    "fig.savefig('ELMoConvergence.png', format='png')\n",
    "fig.savefig('ELMoConvergence.svg', format='svg')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "def create_download_link(title = \"Download file\", filename = \"data.csv\"):  \n",
    "    html = '<a href={filename}>{title}</a>'\n",
    "    html = html.format(title=title,filename=filename)\n",
    "    return HTML(html)\n",
    "\n",
    "create_download_link(filename='ELMoConvergence.svg')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}