{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.util import deprecation\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import models, layers, backend, metrics\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# set random seed\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# other setup\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "pd.set_option('max_colwidth', 50)\n",
    "pio.templates.default = \"presentation\"\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load data\n",
    "data = pd.read_csv('../data/malicious_data.csv')\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "val_size = 0.2\n",
    "train_data, val_data = train_test_split(data, test_size=val_size, stratify=data['label'], random_state=0)\n",
    "fig = go.Figure([go.Pie(labels=['Train Size', 'Validation Size'], values=[train_data.shape[0], val_data.shape[0]])])\n",
    "fig.update_layout(title='Train and Validation Size')\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = go.Figure([go.Pie(labels=['Good', 'Bad'], values=data.label.value_counts())])\n",
    "fig.update_layout(title='Percentage of Class (Good and Bad)')\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parsed_url(url):\n",
    "    # extract subdomain, domain, and domain suffix from url\n",
    "    # if item == '', fill with '<empty>'\n",
    "    subdomain, domain, domain_suffix = ('<empty>' if extracted == '' else extracted for extracted in tldextract.extract(url))\n",
    "    \n",
    "    return [subdomain, domain, domain_suffix]\n",
    "\n",
    "def extract_url(data):\n",
    "    # parsed url\n",
    "    extract_url_data = [parsed_url(url) for url in data['url']]\n",
    "    extract_url_data = pd.DataFrame(extract_url_data, columns=['subdomain', 'domain', 'domain_suffix'])\n",
    "    \n",
    "    # concat extracted feature with main data\n",
    "    data = data.reset_index(drop=True)\n",
    "    data = pd.concat([data, extract_url_data], axis=1)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_frequent_group(data, n_group):\n",
    "    # get the most frequent\n",
    "    data = data.value_counts().reset_index(name='values')\n",
    "    \n",
    "    # scale log base 10\n",
    "    data['values'] = np.log10(data['values'])\n",
    "    \n",
    "    # calculate total values\n",
    "    # x_column (subdomain / domain / domain_suffix)\n",
    "    x_column = data.columns[1]\n",
    "    data['total_values'] = data[x_column].map(data.groupby(x_column)['values'].sum().to_dict())\n",
    "    \n",
    "    # get n_group data order by highest values\n",
    "    data_group = data.sort_values('total_values', ascending=False).iloc[:, 1].unique()[:n_group]\n",
    "    data = data[data.iloc[:, 1].isin(data_group)]\n",
    "    data = data.sort_values('total_values', ascending=False)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def plot(data, n_group, title):\n",
    "    data = get_frequent_group(data, n_group)\n",
    "    fig = px.bar(data, x=data.columns[1], y='values', color='label')\n",
    "    fig.update_layout(title=title)\n",
    "    fig.show()\n",
    "\n",
    "# extract url\n",
    "data = extract_url(data)\n",
    "train_data = extract_url(train_data)\n",
    "val_data = extract_url(val_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = go.Figure([go.Bar(\n",
    "    x=['domain', 'Subdomain', 'Domain Suffix'], \n",
    "    y = [data.domain.nunique(), data.subdomain.nunique(), data.domain_suffix.nunique()]\n",
    ")])\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot(\n",
    "    data=data.groupby('label')['domain'], \n",
    "    n_group=20, \n",
    "    title='Top 20 Domains Grouped By Labels (Logarithmic Scale)'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data[(data['domain'] == 'google') & (data['label'] == 'bad')].head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot(\n",
    "    data=data.groupby('label')['subdomain'], \n",
    "    n_group=20, \n",
    "    title='Top 20 Sub Domains Grouped By Labels (Logarithmic Scale)'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot(\n",
    "    data=data.groupby('label')['domain_suffix'], \n",
    "    n_group=20, \n",
    "    title='Top 20 Domains Suffix Grouped By Labels (Logarithmic Scale)'\n",
    ")   "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters='', char_level=True, lower=False, oov_token=1)\n",
    "\n",
    "# fit only on training data\n",
    "tokenizer.fit_on_texts(train_data['url'])\n",
    "n_char = len(tokenizer.word_index.keys())\n",
    "\n",
    "train_seq = tokenizer.texts_to_sequences(train_data['url'])\n",
    "val_seq = tokenizer.texts_to_sequences(val_data['url'])\n",
    "\n",
    "print('Before tokenization: ')\n",
    "print(train_data.iloc[0]['url'])\n",
    "print('\\nAfter tokenization: ')\n",
    "print(train_seq[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sequence_length = np.array([len(i) for i in train_seq])\n",
    "sequence_length = np.percentile(sequence_length, 99).astype(int)\n",
    "print(f'Before padding: \\n {train_seq[0]}')\n",
    "train_seq = pad_sequences(train_seq, padding='post', maxlen=sequence_length)\n",
    "val_seq = pad_sequences(val_seq, padding='post', maxlen=sequence_length)\n",
    "print(f'After padding: \\n {train_seq[0]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unique_value = {}\n",
    "for feature in ['subdomain', 'domain', 'domain_suffix']:\n",
    "    # get unique value\n",
    "    label_index = {label: index for index, label in enumerate(train_data[feature].unique())}\n",
    "    \n",
    "    # add unknown label in last index\n",
    "    label_index['<unknown>'] = list(label_index.values())[-1] + 1\n",
    "    \n",
    "    # count unique value\n",
    "    unique_value[feature] = label_index['<unknown>']\n",
    "    \n",
    "    # encode\n",
    "    train_data.loc[:, feature] = [label_index[val] if val in label_index else label_index['<unknown>'] for val in train_data.loc[:, feature]]\n",
    "    val_data.loc[:, feature] = [label_index[val] if val in label_index else label_index['<unknown>'] for val in val_data.loc[:, feature]]\n",
    "    \n",
    "train_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for data in [train_data, val_data]:\n",
    "    data.loc[:, 'label'] = [0 if i == 'good' else 1 for i in data.loc[:, 'label']]\n",
    "    \n",
    "train_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def convolution_block(x):\n",
    "    conv_3_layer = layers.Conv1D(64, 3, padding='same', activation='elu')(x)\n",
    "    conv_5_layer = layers.Conv1D(64, 5, padding='same', activation='elu')(x)\n",
    "    conv_layer = layers.concatenate([x, conv_3_layer, conv_5_layer])\n",
    "    conv_layer = layers.Flatten()(conv_layer)\n",
    "    return conv_layer\n",
    "\n",
    "def embedding_block(unique_value, size, name):\n",
    "    input_layer = layers.Input(shape=(1,), name=name + '_input')\n",
    "    embedding_layer = layers.Embedding(unique_value, size, input_length=1)(input_layer)\n",
    "    return input_layer, embedding_layer\n",
    "\n",
    "def create_model(sequence_length, n_char, unique_value):\n",
    "    input_layer = []\n",
    "    \n",
    "    # sequence input layer\n",
    "    sequence_input_layer = layers.Input(shape=(sequence_length,), name='url_input')\n",
    "    input_layer.append(sequence_input_layer)\n",
    "    \n",
    "    # convolution block\n",
    "    char_embedding = layers.Embedding(n_char + 1, 32, input_length=sequence_length)(sequence_input_layer)\n",
    "    conv_layer = convolution_block(char_embedding)\n",
    "    \n",
    "    # entity embedding\n",
    "    entity_embedding = []\n",
    "    for key, n in unique_value.items():\n",
    "        size = 4\n",
    "        input_l, embedding_l = embedding_block(n + 1, size, key)\n",
    "        embedding_l = layers.Reshape(target_shape=(size,))(embedding_l)\n",
    "        input_layer.append(input_l)\n",
    "        entity_embedding.append(embedding_l)\n",
    "        \n",
    "    # concat all layer\n",
    "    fc_layer = layers.concatenate([conv_layer, *entity_embedding])\n",
    "    fc_layer = layers.Dropout(rate=0.5)(fc_layer)\n",
    "    \n",
    "    # dense layer\n",
    "    fc_layer = layers.Dense(128, activation='elu')(fc_layer)\n",
    "    fc_layer = layers.Dropout(rate=0.2)(fc_layer)\n",
    "    \n",
    "    # output layer\n",
    "    output_layer = layers.Dense(1, activation='sigmoid')(fc_layer)\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[metrics.Precision(), metrics.Recall()])\n",
    "    return model\n",
    "\n",
    "# reset session\n",
    "backend.clear_session()\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# create model\n",
    "model = create_model(sequence_length, n_char, unique_value)\n",
    "\n",
    "# show model architecture\n",
    "plot_model(model, to_file='model.png')\n",
    "model_image = mpimg.imread('model.png')\n",
    "plt.figure(figsize=(75, 75))\n",
    "plt.imshow(model_image)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create train data\n",
    "train_x = [train_seq, train_data['subdomain'], train_data['domain'], train_data['domain_suffix']]\n",
    "train_y = train_data['label'].values\n",
    "\n",
    "# model training\n",
    "early_stopping = [EarlyStopping(monitor='val_precision', patience=5, restore_best_weights=True, mode='max')]\n",
    "history = model.fit(train_x, train_y, batch_size=64, epochs=25, verbose=1, validation_split=0.2, shuffle=True, callbacks=early_stopping)\n",
    "model.save('model.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = make_subplots(3, 1, subplot_titles=('loss', 'precision', 'recall'))\n",
    "\n",
    "for index, key in enumerate(['loss', 'precision', 'recall']):\n",
    "    # train score\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(len(history.history[key]))),\n",
    "        y=history.history[key],\n",
    "        mode='lines+markers',\n",
    "        name=key\n",
    "    ), index + 1, 1)\n",
    "    \n",
    "    # val score\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(len(history.history[f'val_{key}']))),\n",
    "        y=history.history[f'val_{key}'],\n",
    "        mode='lines+markers',\n",
    "        name=f'val {key}'\n",
    "    ), index + 1, 1)\n",
    "\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "val_x = [val_seq, val_data['subdomain'], val_data['domain'], val_data['domain_suffix']]\n",
    "val_y = val_data['label'].values\n",
    "\n",
    "val_pred = model.predict(val_x)\n",
    "val_pred = np.where(val_pred[:, 0] >= 0.5, 1, 0)\n",
    "print(f'Validation Data:\\n{val_data.label.value_counts()}')\n",
    "print(f'\\n\\nConfusion Matrix:\\n{confusion_matrix(val_y, val_pred)}')\n",
    "print(f'\\n\\nClassification Report:\\n{classification_report(val_y, val_pred)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}