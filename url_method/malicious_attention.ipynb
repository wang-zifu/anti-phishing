{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('./data/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (6,6)\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "from keras.layers import Dense, Input, LSTM, Bidirectional, Activation, Conv1D, GRU, TimeDistributed\n",
    "from keras.layers import Dropout, Embedding, GlobalMaxPooling1D, MaxPooling1D, Add, Flatten, SpatialDropout1D\n",
    "from keras.layers import GlobalAveragePooling1D, BatchNormalization, concatenate\n",
    "from keras.layers import Reshape, merge, Concatenate, Lambda, Average\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.initializers import Constant\n",
    "from keras.layers.merge import add\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras import initializers, regularizers\n",
    "from keras import optimizers\n",
    "from keras.engine.topology import Layer\n",
    "from keras import constraints\n",
    "\n",
    "############################################## \n",
    "\"\"\"\n",
    "# ATTENTION LAYER\n",
    "Cite these works \n",
    "1. Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "\"Hierarchical Attention Networks for Document Classification\"\n",
    "accepted in NAACL 2016\n",
    "2. Winata, et al. https://arxiv.org/abs/1805.12307\n",
    "\"Attention-Based LSTM for Psychological Stress Detection from Spoken Language Using Distant Supervision.\" \n",
    "accepted in ICASSP 2018\n",
    "Using a context vector to assist the attention\n",
    "* How to use:\n",
    "Put return_sequences=True on the top of an RNN Layer (GRU/LSTM/SimpleRNN).\n",
    "The dimensions are inferred based on the output shape of the RNN.\n",
    "Example:\n",
    "\tmodel.add(LSTM(64, return_sequences=True))\n",
    "\tmodel.add(AttentionWithContext())\n",
    "\tmodel.add(Addition())\n",
    "\t# next add a Dense layer (for classification/regression) or whatever...\n",
    "\"\"\"\n",
    "##############################################\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "\t\"\"\"\n",
    "\tWrapper for dot product operation, in order to be compatible with both\n",
    "\tTheano and Tensorflow\n",
    "\tArgs:\n",
    "\t\tx (): input\n",
    "\t\tkernel (): weights\n",
    "\tReturns:\n",
    "\t\"\"\"\n",
    "\tif K.backend() == 'tensorflow':\n",
    "\t\treturn K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "\telse:\n",
    "\t\treturn K.dot(x, kernel)\n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "\t\"\"\"\n",
    "\tAttention operation, with a context/query vector, for temporal data.\n",
    "\tSupports Masking.\n",
    "\tfollows these equations:\n",
    "\t\n",
    "\t(1) u_t = tanh(W h_t + b)\n",
    "\t(2) \\alpha_t = \\frac{exp(u^T u)}{\\sum_t(exp(u_t^T u))}, this is the attention weight\n",
    "\t(3) v_t = \\alpha_t * h_t, v in time t\n",
    "\t# Input shape\n",
    "\t\t3D tensor with shape: `(samples, steps, features)`.\n",
    "\t# Output shape\n",
    "\t\t3D tensor with shape: `(samples, steps, features)`.\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self,\n",
    "\t\t\t\t W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "\t\t\t\t W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "\t\t\t\t bias=True, **kwargs):\n",
    "\n",
    "\t\tself.supports_masking = True\n",
    "\t\tself.init = initializers.get('glorot_uniform')\n",
    "\n",
    "\t\tself.W_regularizer = regularizers.get(W_regularizer)\n",
    "\t\tself.u_regularizer = regularizers.get(u_regularizer)\n",
    "\t\tself.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "\t\tself.W_constraint = constraints.get(W_constraint)\n",
    "\t\tself.u_constraint = constraints.get(u_constraint)\n",
    "\t\tself.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "\t\tself.bias = bias\n",
    "\t\tsuper(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "\tdef build(self, input_shape):\n",
    "\t\tassert len(input_shape) == 3\n",
    "\n",
    "\t\tself.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n",
    "\t\t\t\t\t\t\t\t initializer=self.init,\n",
    "\t\t\t\t\t\t\t\t name='{}_W'.format(self.name),\n",
    "\t\t\t\t\t\t\t\t regularizer=self.W_regularizer,\n",
    "\t\t\t\t\t\t\t\t constraint=self.W_constraint)\n",
    "\t\tif self.bias:\n",
    "\t\t\tself.b = self.add_weight(shape=(input_shape[-1],),\n",
    "\t\t\t\t\t\t\t\t\t initializer='zero',\n",
    "\t\t\t\t\t\t\t\t\t name='{}_b'.format(self.name),\n",
    "\t\t\t\t\t\t\t\t\t regularizer=self.b_regularizer,\n",
    "\t\t\t\t\t\t\t\t\t constraint=self.b_constraint)\n",
    "\n",
    "\t\tself.u = self.add_weight(shape=(input_shape[-1],),\n",
    "\t\t\t\t\t\t\t\t initializer=self.init,\n",
    "\t\t\t\t\t\t\t\t name='{}_u'.format(self.name),\n",
    "\t\t\t\t\t\t\t\t regularizer=self.u_regularizer,\n",
    "\t\t\t\t\t\t\t\t constraint=self.u_constraint)\n",
    "\n",
    "\t\tsuper(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "\tdef compute_mask(self, input, input_mask=None):\n",
    "\t\t# do not pass the mask to the next layers\n",
    "\t\treturn None\n",
    "\n",
    "\tdef call(self, x, mask=None):\n",
    "\t\tuit = dot_product(x, self.W)\n",
    "\n",
    "\t\tif self.bias:\n",
    "\t\t\tuit += self.b\n",
    "\n",
    "\t\tuit = K.tanh(uit)\n",
    "\t\tait = dot_product(uit, self.u)\n",
    "\n",
    "\t\ta = K.exp(ait)\n",
    "\n",
    "\t\t# apply mask after the exp. will be re-normalized next\n",
    "\t\tif mask is not None:\n",
    "\t\t\t# Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "\t\t\ta *= K.cast(mask, K.floatx())\n",
    "\n",
    "\t\t# in some cases especially in the early stages of training the sum may be almost zero and this results in NaN's. \n",
    "\t\t# Should add a small epsilon as the workaround\n",
    "\t\t# a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "\t\ta /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "\t\ta = K.expand_dims(a)\n",
    "\t\tweighted_input = x * a\n",
    "\t\t\n",
    "\t\treturn weighted_input\n",
    "\n",
    "\tdef compute_output_shape(self, input_shape):\n",
    "\t\treturn input_shape[0], input_shape[1], input_shape[2]\n",
    "\t\n",
    "class Addition(Layer):\n",
    "\t\"\"\"\n",
    "\tThis layer is supposed to add of all activation weight.\n",
    "\tWe split this from AttentionWithContext to help us getting the activation weights\n",
    "\tfollows this equation:\n",
    "\t(1) v = \\sum_t(\\alpha_t * h_t)\n",
    "\t\n",
    "\t# Input shape\n",
    "\t\t3D tensor with shape: `(samples, steps, features)`.\n",
    "\t# Output shape\n",
    "\t\t2D tensor with shape: `(samples, features)`.\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, **kwargs):\n",
    "\t\tsuper(Addition, self).__init__(**kwargs)\n",
    "\n",
    "\tdef build(self, input_shape):\n",
    "\t\tself.output_dim = input_shape[-1]\n",
    "\t\tsuper(Addition, self).build(input_shape)\n",
    "\n",
    "\tdef call(self, x):\n",
    "\t\treturn K.sum(x, axis=1)\n",
    "\n",
    "\tdef compute_output_shape(self, input_shape):\n",
    "\t\treturn (input_shape[0], self.output_dim)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Loading data...')\n",
    "#thay đổi dữ liệu đầu vào tùy thuộc vào tập dữ liệu cần chạy\n",
    "path ='/kaggle/input/url-malicious-lstm/2k5.csv'\n",
    "# load data\n",
    "df =  pd.read_csv(path)\n",
    "\n",
    "print(f'Data size: {df.shape}')\n",
    "\n",
    "print(df['label'].value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "samples = df.url\n",
    "labels = df.label\n",
    "max_chars = 20000\n",
    "maxlen = 128"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_chars, char_level=True)\n",
    "tokenizer.fit_on_texts(samples)\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "labels = [1 if i=='bad' else 0 for i in labels]\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_samples = int(len(samples) * 0.95)\n",
    "validation_samples = int(len(labels) * 0.05)\n",
    "print(training_samples, validation_samples)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = data[:training_samples]\n",
    "y = labels[:training_samples]\n",
    "x_test = data[training_samples: training_samples + validation_samples]\n",
    "y_test = labels[training_samples: training_samples + validation_samples]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_chars = len(tokenizer.word_index)+1\n",
    "embedding_vector_length = 128"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "def make_model(n_batch,num_chars, embedding_vector_length, maxlen):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_chars, embedding_vector_length, input_length=maxlen))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(Bidirectional(LSTM(64,  \n",
    "                                 dropout=0.25, \n",
    "                                 recurrent_dropout=0.25,return_sequences=True)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Bidirectional(LSTM(64,  \n",
    "                                 dropout=0.25, \n",
    "                                 recurrent_dropout=0.25, \n",
    "                                 return_sequences=True)))\n",
    "    model.add(AttentionWithContext())\n",
    "    model.add(Addition())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.summary()\n",
    "    start = time.time()\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy', f1_m, recall_m, precision_m])\n",
    "    print(\"Compilation Time : \", time.time() - start)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_model_no_attention(n_batch,num_chars, embedding_vector_length, maxlen):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_chars, embedding_vector_length, input_length=maxlen))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(Bidirectional(LSTM(128, dropout=0.2,recurrent_dropout=0.2)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.summary()\n",
    "    start = time.time()\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy', f1_m, recall_m, precision_m])\n",
    "    print(\"Compilation Time : \", time.time() - start)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "def evaluate_model_with_n_layers(n_batch, x, y, x_test, y_test, num_chars, embedding_vector_length, maxlen,callbacks_list):\n",
    "    model = make_model(n_batch,num_chars, embedding_vector_length, maxlen)\n",
    "    hist=model.fit(x, y,\n",
    "                epochs=100,\n",
    "                batch_size=n_batch,\n",
    "                callbacks=callbacks_list,\n",
    "                validation_split=0.20,\n",
    "                shuffle=True,\n",
    "                verbose=1\n",
    "                )\n",
    "    test_acc = model.evaluate(x_test,y_test, verbose=0)\n",
    "    return hist,test_acc\n",
    "\n",
    "\n",
    "all_history = list ()\n",
    "n_layers = 2\n",
    "num_batch_size = [32]\n",
    "for n_batch in num_batch_size:\n",
    "    file_path = \"BestModel_data_2k5_\"+str(n_batch)+\".hdf5\"\n",
    "\n",
    "    callbacks_list = [\n",
    "        ModelCheckpoint(\n",
    "            file_path,\n",
    "            monitor='val_loss',\n",
    "            verbose=1,\n",
    "            save_best_only=True,\n",
    "            mode='min'\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            min_delta=0,\n",
    "            patience=5, \n",
    "            verbose=1,\n",
    "            mode='auto',\n",
    "        )\n",
    "    ]\n",
    "    print('Starting test....')\n",
    "    history, result = evaluate_model_with_n_layers(n_batch, x, y, x_test, y_test, num_chars, embedding_vector_length, maxlen,callbacks_list)\n",
    "    print('Batch_Size =%d:' % n_batch)\n",
    "    print(result)\n",
    "    all_history.append(history)\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.plot(history.history['val_accuracy'],label='val_accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}